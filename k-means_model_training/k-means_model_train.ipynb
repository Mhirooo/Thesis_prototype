{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe98ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 1: Install Dependencies\n",
    "# %pip install -q sentence-transformers scikit-learn chromadb matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad95f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Cell 3: Load the full dataset\n",
    "data = pd.read_csv(\"resume_screening_train.csv\")\n",
    "\n",
    "# Use only 25% of the full dataset\n",
    "data_subset = data.sample(frac=0.25, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the 25% subset into train and test (50/50 split)\n",
    "train, test = train_test_split(data_subset, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reset indices\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# Print information\n",
    "print(f\"Total dataset: {len(data)} samples\")\n",
    "print(f\"Subset used (25%): {len(data_subset)} samples\")\n",
    "print(f\"Training data: {len(train)} samples ({len(train)/len(data)*100:.1f}% of total)\")\n",
    "print(f\"Testing data: {len(test)} samples ({len(test)/len(data)*100:.1f}% of total)\")\n",
    "\n",
    "# Concatenate Role and Job Description\n",
    "train['role_jobdesc'] = train['Role'] + \" \" + train['Job_Description']\n",
    "test['role_jobdesc'] = test['Role'] + \" \" + test['Job_Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8264a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load SBERT and Encode Training Data\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode training data\n",
    "print(\"Encoding training data...\")\n",
    "role_jobdesc_embeddings_train = model.encode(train['role_jobdesc'].tolist(), show_progress_bar=True)\n",
    "resume_embeddings_train = model.encode(train['Resume'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# For clustering, use only job description embeddings\n",
    "X_train_embeddings = role_jobdesc_embeddings_train\n",
    "\n",
    "print(f\"Training embeddings shape: {X_train_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc64b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Elbow Method to Determine Best k (using training data only)\n",
    "inertias = []\n",
    "k_values = range(2, min(20, len(X_train_embeddings)//2))\n",
    "\n",
    "print(f\"Testing k values from {min(k_values)} to {max(k_values)}...\")\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_train_embeddings)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k (Training Data)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Calculate silhouette scores on training data\n",
    "silhouette_scores = []\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_train_embeddings)\n",
    "    score = silhouette_score(X_train_embeddings, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, silhouette_scores, marker='s', color='orange')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Different k (Training Data)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(silhouette_scores)]\n",
    "print(f\"Best k selected: {best_k} (Training Silhouette Score: {max(silhouette_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc284a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train Final K-Means Model (on training data only)\n",
    "print(f\"Training K-means with k={best_k} on {len(X_train_embeddings)} training samples...\")\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "kmeans.fit(X_train_embeddings)\n",
    "print(\"K-means training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Data Cluster Visualization\n",
    "pca = PCA(n_components=2)\n",
    "train_2d = pca.fit_transform(X_train_embeddings)\n",
    "\n",
    "# Get cluster assignments for training data\n",
    "train_labels = kmeans.predict(X_train_embeddings)\n",
    "centroids_2d = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "# Plot training data\n",
    "plt.figure(figsize=(10,8))\n",
    "scatter = plt.scatter(train_2d[:,0], train_2d[:,1], c=train_labels, cmap='tab10', alpha=0.6, s=30)\n",
    "plt.scatter(centroids_2d[:,0], centroids_2d[:,1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title(f\"Training Data Cluster Visualization\\nk={best_k} clusters\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.colorbar(scatter)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print training cluster distribution\n",
    "unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "print(\"\\nTraining Cluster Distribution:\")\n",
    "for cluster_id, count in zip(unique_train, counts_train):\n",
    "    print(f\"Cluster {cluster_id}: {count} samples ({count/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "# --- Euclidean Distance Analysis ---\n",
    "# Distances from each point to each centroid\n",
    "distances = cdist(X_train_embeddings, kmeans.cluster_centers_, 'euclidean')\n",
    "\n",
    "# Average distance of points to their assigned centroid (intra-cluster cohesion)\n",
    "print(\"\\nAverage Intra-cluster Euclidean Distances:\")\n",
    "for i in range(best_k):\n",
    "    cluster_points = distances[train_labels == i, i]\n",
    "    print(f\"Cluster {i}: {cluster_points.mean():.4f}\")\n",
    "\n",
    "# Distance between centroids (inter-cluster separation)\n",
    "centroid_distances = cdist(kmeans.cluster_centers_, kmeans.cluster_centers_, 'euclidean')\n",
    "print(\"\\nCentroid-to-centroid Euclidean Distances:\")\n",
    "print(centroid_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b365ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Data Evaluation\n",
    "train_silhouette = silhouette_score(X_train_embeddings, train_labels)\n",
    "print(f\"Training Silhouette Score: {train_silhouette:.4f}\")\n",
    "print(f\"Training Inertia (Within-cluster sum of squares): {kmeans.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb35565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Testing Phase (predict clusters for test job descriptions)\n",
    "print(\"TESTING PHASE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Encode test data\n",
    "print(\"Encoding test data...\")\n",
    "role_jobdesc_embeddings_test = model.encode(test['role_jobdesc'].tolist(), show_progress_bar=True)\n",
    "resume_embeddings_test = model.encode(test['Resume'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Predict clusters for test data (using trained model)\n",
    "test_labels = kmeans.predict(role_jobdesc_embeddings_test)\n",
    "test_silhouette = silhouette_score(role_jobdesc_embeddings_test, test_labels)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Test Silhouette Score: {test_silhouette:.4f}\")\n",
    "print(f\"Training Silhouette Score: {train_silhouette:.4f}\")\n",
    "print(f\"Difference (Train - Test): {train_silhouette - test_silhouette:.4f}\")\n",
    "\n",
    "# Check for overfitting/underfitting\n",
    "if abs(train_silhouette - test_silhouette) < 0.05:\n",
    "    print(\"✓ Good generalization (small train-test gap)\")\n",
    "elif train_silhouette > test_silhouette + 0.1:\n",
    "    print(\"⚠ Possible overfitting (train much better than test)\")\n",
    "else:\n",
    "    print(\"ℹ Model performance difference within acceptable range\")\n",
    "\n",
    "# Test cluster distribution\n",
    "unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "print(f\"\\nTest Cluster Distribution:\")\n",
    "for cluster_id, count in zip(unique_test, counts_test):\n",
    "    print(f\"Cluster {cluster_id}: {count} samples ({count/len(test_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27524d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Test Set Cluster Visualization\n",
    "test_2d = pca.transform(role_jobdesc_embeddings_test)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training data plot\n",
    "plt.subplot(1, 3, 1)\n",
    "scatter1 = plt.scatter(train_2d[:,0], train_2d[:,1], c=train_labels, cmap='tab10', alpha=0.6, s=30)\n",
    "plt.scatter(centroids_2d[:,0], centroids_2d[:,1], c='red', marker='X', s=200)\n",
    "plt.title(f\"Training Data Clusters\\n(Silhouette: {train_silhouette:.3f})\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "# Test data plot\n",
    "plt.subplot(1, 3, 2)\n",
    "scatter2 = plt.scatter(test_2d[:,0], test_2d[:,1], c=test_labels, cmap='tab10', alpha=0.6, s=30)\n",
    "plt.scatter(centroids_2d[:,0], centroids_2d[:,1], c='red', marker='X', s=200)\n",
    "plt.title(f\"Test Data Clusters\\n(Silhouette: {test_silhouette:.3f})\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "\n",
    "# Combined plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(train_2d[:,0], train_2d[:,1], c=train_labels, cmap='tab10', alpha=0.4, s=20, label='Train')\n",
    "plt.scatter(test_2d[:,0], test_2d[:,1], c=test_labels, cmap='tab10', alpha=0.8, s=30, marker='^', label='Test')\n",
    "plt.scatter(centroids_2d[:,0], centroids_2d[:,1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title(\"Training vs Test Data\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2caae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Comprehensive Test Evaluation\n",
    "print(\"COMPREHENSIVE TEST EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cosine similarity comparison between train and test\n",
    "train_similarities = [\n",
    "    cosine_similarity(resume_embeddings_train[i].reshape(1, -1), \n",
    "                     role_jobdesc_embeddings_train[i].reshape(1, -1))[0][0]\n",
    "    for i in range(len(resume_embeddings_train))\n",
    "]\n",
    "\n",
    "test_similarities = [\n",
    "    cosine_similarity(resume_embeddings_test[i].reshape(1, -1), \n",
    "                     role_jobdesc_embeddings_test[i].reshape(1, -1))[0][0]\n",
    "    for i in range(len(resume_embeddings_test))\n",
    "]\n",
    "\n",
    "print(\"Cosine Similarity Comparison (Resume ↔ Job Description):\")\n",
    "print(f\"Train Avg: {np.mean(train_similarities):.4f} ± {np.std(train_similarities):.4f}\")\n",
    "print(f\"Test Avg:  {np.mean(test_similarities):.4f} ± {np.std(test_similarities):.4f}\")\n",
    "\n",
    "# Sample examples from test set\n",
    "print(f\"\\nSample Test Examples:\")\n",
    "sample_indices = random.sample(range(len(test_similarities)), min(5, len(test_similarities)))\n",
    "for idx in sample_indices:\n",
    "    cluster_id = test_labels[idx]\n",
    "    sim_score = test_similarities[idx]\n",
    "    print(f\"Test Sample {idx} | Cluster: {cluster_id} | Similarity: {sim_score:.4f}\")\n",
    "\n",
    "# Test similarity by cluster\n",
    "print(f\"\\nTest Similarity by Cluster:\")\n",
    "for cluster_id in range(best_k):\n",
    "    cluster_mask = test_labels == cluster_id\n",
    "    if np.any(cluster_mask):\n",
    "        cluster_sims = np.array(test_similarities)[cluster_mask]\n",
    "        print(f\"Cluster {cluster_id}: {np.mean(cluster_sims):.4f} ± {np.std(cluster_sims):.4f} (n={len(cluster_sims)})\")\n",
    "\n",
    "# Distance to centroids analysis\n",
    "print(f\"\\nDistance to Centroids Analysis:\")\n",
    "train_distances = []\n",
    "for i in range(len(X_train_embeddings)):\n",
    "    distances = [np.linalg.norm(X_train_embeddings[i] - centroid) for centroid in kmeans.cluster_centers_]\n",
    "    train_distances.append(np.min(distances))\n",
    "\n",
    "test_distances = []\n",
    "for i in range(len(role_jobdesc_embeddings_test)):\n",
    "    distances = [np.linalg.norm(role_jobdesc_embeddings_test[i] - centroid) for centroid in kmeans.cluster_centers_]\n",
    "    test_distances.append(np.min(distances))\n",
    "\n",
    "print(f\"Train Avg Distance to Nearest Centroid: {np.mean(train_distances):.4f} ± {np.std(train_distances):.4f}\")\n",
    "print(f\"Test Avg Distance to Nearest Centroid: {np.mean(test_distances):.4f} ± {np.std(test_distances):.4f}\")\n",
    "\n",
    "if np.mean(test_distances) <= np.mean(train_distances) * 1.2:\n",
    "    print(\"✓ Test data fits well within learned cluster structure\")\n",
    "else:\n",
    "    print(\"⚠ Test data seems more distant from centroids than training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5652ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Save Artifacts and Final Summary\n",
    "# Save the trained model\n",
    "joblib.dump(kmeans, \"kmeans_model.pkl\")\n",
    "print(\"✓ KMeans model saved as 'kmeans_model.pkl'\")\n",
    "\n",
    "# Print final comprehensive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TRAINING & TESTING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset split: 50% used (40% train, 10% test), 50% unused\")\n",
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Testing samples: {len(test)}\")\n",
    "print(f\"Optimal number of clusters: {best_k}\")\n",
    "print(f\"Training silhouette score: {train_silhouette:.4f}\")\n",
    "print(f\"Test silhouette score: {test_silhouette:.4f}\")\n",
    "print(f\"Generalization gap: {abs(train_silhouette - test_silhouette):.4f}\")\n",
    "print(f\"Train avg cosine similarity: {np.mean(train_similarities):.4f}\")\n",
    "print(f\"Test avg cosine similarity: {np.mean(test_similarities):.4f}\")\n",
    "print(f\"Model and all artifacts saved successfully!\")\n",
    "\n",
    "# Performance assessment\n",
    "if test_silhouette > 0.3:\n",
    "    print(\"✓ Good clustering performance on test data\")\n",
    "elif test_silhouette > 0.1:\n",
    "    print(\"~ Moderate clustering performance on test data\")\n",
    "else:\n",
    "    print(\"⚠ Low clustering performance - consider more clusters or different features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
